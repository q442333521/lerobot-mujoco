{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 部署训练好的策略\n",
    "\n",
    "<img src=\"./media/rollout.gif\" width=\"480\" height=\"360\">\n",
    "\n",
    "**中文教程：如何在仿真环境中部署训练好的机器人策略**\n",
    "\n",
    "本教程将指导您如何加载训练好的ACT（Action Chunking with Transformers）策略，并在MuJoCo仿真环境中进行部署测试。\n",
    "\n",
    "## 学习目标\n",
    "- 理解策略部署的基本流程\n",
    "- 学会加载预训练模型\n",
    "- 掌握环境与策略的交互方式\n",
    "- 能够运行完整的部署流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DISPLAY设置为: :0\n",
      "✓ MUJOCO_GL: egl (GPU硬件加速)\n",
      "✓ NVIDIA GPU优化已启用\n",
      "✓ OpenGL性能优化已启用\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - 设置环境变量（必须第一个运行）\n",
    "import os\n",
    "\n",
    "# 1. 设置DISPLAY - 用于图形显示\n",
    "# 在Linux系统中，DISPLAY环境变量告诉程序在哪里显示图形界面\n",
    "os.environ['DISPLAY'] = ':0'\n",
    "os.environ['XAUTHORITY'] = os.path.expanduser('~/.Xauthority')\n",
    "print(f\"✓ DISPLAY设置为: {os.environ['DISPLAY']}\")\n",
    "\n",
    "# 2. 强制使用GPU渲染（关键！）\n",
    "# MUJOCO_GL='egl' 使用EGL后端进行GPU硬件加速渲染\n",
    "# 这比CPU渲染快很多，特别是对于复杂的3D场景\n",
    "os.environ['MUJOCO_GL'] = 'egl'  # EGL后端GPU加速\n",
    "print(f\"✓ MUJOCO_GL: egl (GPU硬件加速)\")\n",
    "\n",
    "# 3. NVIDIA GPU优化\n",
    "# 关闭垂直同步可以避免帧率限制，提高渲染性能\n",
    "os.environ['__GL_SYNC_TO_VBLANK'] = '0'  # 关闭垂直同步\n",
    "os.environ['__GL_YIELD'] = 'NOTHING'      # 减少CPU等待\n",
    "print(\"✓ NVIDIA GPU优化已启用\")\n",
    "\n",
    "# 4. OpenGL性能优化\n",
    "# 关闭抗锯齿和各向异性过滤可以显著提高渲染速度\n",
    "os.environ['__GL_FSAA_MODE'] = '0'        # 关闭抗锯齿\n",
    "os.environ['__GL_LOG_MAX_ANISO'] = '0'    # 关闭各向异性过滤\n",
    "print(\"✓ OpenGL性能优化已启用\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
    "import numpy as np\n",
    "from lerobot.common.datasets.utils import write_json, serialize_dict\n",
    "from lerobot.common.policies.act.configuration_act import ACTConfig\n",
    "from lerobot.common.policies.act.modeling_act import ACTPolicy\n",
    "from lerobot.configs.types import FeatureType\n",
    "from lerobot.common.datasets.factory import resolve_delta_timestamps\n",
    "from lerobot.common.datasets.utils import dataset_to_policy_features\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'  # 指定使用GPU进行计算，如果GPU不可用可以改为'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Device 'None' is not available. Switching to 'cuda'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from local directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ACTPolicy(\n",
       "  (normalize_inputs): Normalize(\n",
       "    (buffer_observation_image): ParameterDict(\n",
       "        (mean): Parameter containing: [torch.cuda.FloatTensor of size 3x1x1 (cuda:0)]\n",
       "        (std): Parameter containing: [torch.cuda.FloatTensor of size 3x1x1 (cuda:0)]\n",
       "    )\n",
       "    (buffer_observation_state): ParameterDict(\n",
       "        (mean): Parameter containing: [torch.cuda.FloatTensor of size 6 (cuda:0)]\n",
       "        (std): Parameter containing: [torch.cuda.FloatTensor of size 6 (cuda:0)]\n",
       "    )\n",
       "  )\n",
       "  (normalize_targets): Normalize(\n",
       "    (buffer_action): ParameterDict(\n",
       "        (mean): Parameter containing: [torch.cuda.FloatTensor of size 7 (cuda:0)]\n",
       "        (std): Parameter containing: [torch.cuda.FloatTensor of size 7 (cuda:0)]\n",
       "    )\n",
       "  )\n",
       "  (unnormalize_outputs): Unnormalize(\n",
       "    (buffer_action): ParameterDict(\n",
       "        (mean): Parameter containing: [torch.cuda.FloatTensor of size 7 (cuda:0)]\n",
       "        (std): Parameter containing: [torch.cuda.FloatTensor of size 7 (cuda:0)]\n",
       "    )\n",
       "  )\n",
       "  (model): ACT(\n",
       "    (vae_encoder): ACTEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x ACTEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=3200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=3200, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (vae_encoder_cls_embed): Embedding(1, 512)\n",
       "    (vae_encoder_robot_state_input_proj): Linear(in_features=6, out_features=512, bias=True)\n",
       "    (vae_encoder_action_input_proj): Linear(in_features=7, out_features=512, bias=True)\n",
       "    (vae_encoder_latent_output_proj): Linear(in_features=512, out_features=64, bias=True)\n",
       "    (backbone): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoder): ACTEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x ACTEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=3200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=3200, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (decoder): ACTDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): ACTDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=3200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=3200, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (encoder_robot_state_input_proj): Linear(in_features=6, out_features=512, bias=True)\n",
       "    (encoder_latent_input_proj): Linear(in_features=32, out_features=512, bias=True)\n",
       "    (encoder_img_feat_input_proj): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (encoder_1d_feature_pos_embed): Embedding(2, 512)\n",
       "    (encoder_cam_feat_pos_embed): ACTSinusoidalPositionEmbedding2d()\n",
       "    (decoder_pos_embed): Embedding(10, 512)\n",
       "    (action_head): Linear(in_features=512, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 加载数据集元数据 - 包含了数据集的统计信息和特征描述\n",
    "dataset_metadata = LeRobotDatasetMetadata(\"omy_pnp\", root='./demo_data')  # 从demo_data目录加载omy_pnp数据集的元数据\n",
    "\n",
    "# 2. 特征处理 - 将数据集特征转换为策略可用的格式\n",
    "features = dataset_to_policy_features(dataset_metadata.features)  # 将数据集特征转换为策略可用的格式\n",
    "\n",
    "# 3. 区分输入和输出特征\n",
    "# 输出特征是机器人的动作（如关节角度）\n",
    "output_features = {key: ft for key, ft in features.items() if ft.type is FeatureType.ACTION}\n",
    "# 输入特征是观察值（如图像、机器人状态等）\n",
    "input_features = {key: ft for key, ft in features.items() if key not in output_features}\n",
    "# 移除手腕相机图像特征（本例中不使用）\n",
    "input_features.pop(\"observation.wrist_image\")\n",
    "\n",
    "# 4. 创建ACT策略配置\n",
    "# ACTConfig是Action Chunking with Transformers的配置类\n",
    "# chunk_size=10: 每次预测10个时间步的动作\n",
    "# n_action_steps=1: 每次执行1个动作步骤\n",
    "# temporal_ensemble_coeff=0.9: 时间集成系数，使动作预测更平滑\n",
    "cfg = ACTConfig(input_features=input_features, output_features=output_features, chunk_size= 10, n_action_steps=1, temporal_ensemble_coeff = 0.9)\n",
    "\n",
    "# 5. 处理时间戳信息\n",
    "delta_timestamps = resolve_delta_timestamps(cfg, dataset_metadata)\n",
    "\n",
    "# 6. 实例化策略模型并加载预训练权重\n",
    "# from_pretrained方法从本地目录加载预训练模型\n",
    "policy = ACTPolicy.from_pretrained('./ckpt/act_y', config = cfg, dataset_stats=dataset_metadata.stats)  # 从本地目录加载预训练模型\n",
    "\n",
    "# 7. 将策略模型移动到指定设备（GPU）\n",
    "policy.to(device)  # 将模型移到GPU上以加速计算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------------------------------------\n",
      "name:[Tabletop] dt:[0.002] HZ:[500]\n",
      " n_qpos:[24] n_qvel:[22] n_qacc:[22] n_ctrl:[10]\n",
      " integrator:[IMPLICITFAST]\n",
      "\n",
      "n_body:[21]\n",
      " [0/21] [world] mass:[0.00]kg\n",
      " [1/21] [front_object_table] mass:[1.00]kg\n",
      " [2/21] [camera] mass:[0.00]kg\n",
      " [3/21] [camera2] mass:[0.00]kg\n",
      " [4/21] [camera3] mass:[0.00]kg\n",
      " [5/21] [link1] mass:[2.06]kg\n",
      " [6/21] [link2] mass:[3.68]kg\n",
      " [7/21] [link3] mass:[2.39]kg\n",
      " [8/21] [link4] mass:[1.40]kg\n",
      " [9/21] [link5] mass:[1.40]kg\n",
      " [10/21] [link6] mass:[0.65]kg\n",
      " [11/21] [camera_center] mass:[0.00]kg\n",
      " [12/21] [tcp_link] mass:[0.32]kg\n",
      " [13/21] [rh_p12_rn_r1] mass:[0.07]kg\n",
      " [14/21] [rh_p12_rn_r2] mass:[0.02]kg\n",
      " [15/21] [rh_p12_rn_l1] mass:[0.07]kg\n",
      " [16/21] [rh_p12_rn_l2] mass:[0.02]kg\n",
      " [17/21] [body_obj_mug_5] mass:[0.00]kg\n",
      " [18/21] [object_mug_5] mass:[0.08]kg\n",
      " [19/21] [body_obj_plate_11] mass:[0.00]kg\n",
      " [20/21] [object_plate_11] mass:[0.10]kg\n",
      "body_total_mass:[13.27]kg\n",
      "\n",
      "n_geom:[83]\n",
      "geom_names:['floor', None, 'front_object_table', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "\n",
      "n_mesh:[79]\n",
      "mesh_names:['base_unit', 'link1', 'link2', 'link3', 'link4', 'link5', 'link6', 'flange', 'base', 'r1', 'r2', 'l1', 'l2', 'mug_5_normalized_0_vis', 'mug_5_normalized_collision_22._coll', 'mug_5_normalized_collision_23._coll', 'mug_5_normalized_collision_21._coll', 'mug_5_normalized_collision_20._coll', 'mug_5_normalized_collision_24._coll', 'mug_5_normalized_collision_30._coll', 'mug_5_normalized_collision_18._coll', 'mug_5_normalized_collision_19._coll', 'mug_5_normalized_collision_31._coll', 'mug_5_normalized_collision_25._coll', 'mug_5_normalized_collision_27._coll', 'mug_5_normalized_collision_26._coll', 'mug_5_normalized_collision_9._coll', 'mug_5_normalized_collision_8._coll', 'mug_5_normalized_collision_6._coll', 'mug_5_normalized_collision_7._coll', 'mug_5_normalized_collision_5._coll', 'mug_5_normalized_collision_4._coll', 'mug_5_normalized_collision_0._coll', 'mug_5_normalized_collision_1._coll', 'mug_5_normalized_collision_3._coll', 'mug_5_normalized_collision_2._coll', 'mug_5_normalized_collision_17._coll', 'mug_5_normalized_collision_16._coll', 'mug_5_normalized_collision_28._coll', 'mug_5_normalized_collision_14._coll', 'mug_5_normalized_collision_15._coll', 'mug_5_normalized_collision_29._coll', 'mug_5_normalized_collision_11._coll', 'mug_5_normalized_collision_10._coll', 'mug_5_normalized_collision_12._coll', 'mug_5_normalized_collision_13._coll', 'plate_11_normalized_0_vis', 'plate_11_normalized_collision_22._coll', 'plate_11_normalized_collision_23._coll', 'plate_11_normalized_collision_21._coll', 'plate_11_normalized_collision_20._coll', 'plate_11_normalized_collision_24._coll', 'plate_11_normalized_collision_30._coll', 'plate_11_normalized_collision_18._coll', 'plate_11_normalized_collision_19._coll', 'plate_11_normalized_collision_31._coll', 'plate_11_normalized_collision_25._coll', 'plate_11_normalized_collision_27._coll', 'plate_11_normalized_collision_26._coll', 'plate_11_normalized_collision_9._coll', 'plate_11_normalized_collision_8._coll', 'plate_11_normalized_collision_6._coll', 'plate_11_normalized_collision_7._coll', 'plate_11_normalized_collision_5._coll', 'plate_11_normalized_collision_4._coll', 'plate_11_normalized_collision_0._coll', 'plate_11_normalized_collision_1._coll', 'plate_11_normalized_collision_3._coll', 'plate_11_normalized_collision_2._coll', 'plate_11_normalized_collision_17._coll', 'plate_11_normalized_collision_16._coll', 'plate_11_normalized_collision_28._coll', 'plate_11_normalized_collision_14._coll', 'plate_11_normalized_collision_15._coll', 'plate_11_normalized_collision_29._coll', 'plate_11_normalized_collision_11._coll', 'plate_11_normalized_collision_10._coll', 'plate_11_normalized_collision_12._coll', 'plate_11_normalized_collision_13._coll']\n",
      "\n",
      "n_joint:[12]\n",
      " [0/12] [joint1] axis:[0. 0. 1.]\n",
      " [1/12] [joint2] axis:[0. 1. 0.]\n",
      " [2/12] [joint3] axis:[0. 1. 0.]\n",
      " [3/12] [joint4] axis:[0. 1. 0.]\n",
      " [4/12] [joint5] axis:[0. 0. 1.]\n",
      " [5/12] [joint6] axis:[0. 1. 0.]\n",
      " [6/12] [rh_r1] axis:[1. 0. 0.]\n",
      " [7/12] [rh_r2] axis:[-1.  0.  0.]\n",
      " [8/12] [rh_l1] axis:[-1.  0.  0.]\n",
      " [9/12] [rh_l2] axis:[1. 0. 0.]\n",
      " [10/12] [None] axis:[0. 0. 1.]\n",
      " [11/12] [None] axis:[0. 0. 1.]\n",
      "\n",
      "n_dof:[22] (=number of rows of Jacobian)\n",
      " [0/22] [None] attached joint:[joint1] body:[link1]\n",
      " [1/22] [None] attached joint:[joint2] body:[link2]\n",
      " [2/22] [None] attached joint:[joint3] body:[link3]\n",
      " [3/22] [None] attached joint:[joint4] body:[link4]\n",
      " [4/22] [None] attached joint:[joint5] body:[link5]\n",
      " [5/22] [None] attached joint:[joint6] body:[link6]\n",
      " [6/22] [None] attached joint:[rh_r1] body:[rh_p12_rn_r1]\n",
      " [7/22] [None] attached joint:[rh_r2] body:[rh_p12_rn_r2]\n",
      " [8/22] [None] attached joint:[rh_l1] body:[rh_p12_rn_l1]\n",
      " [9/22] [None] attached joint:[rh_l2] body:[rh_p12_rn_l2]\n",
      " [10/22] [None] attached joint:[None] body:[body_obj_mug_5]\n",
      " [11/22] [None] attached joint:[None] body:[body_obj_mug_5]\n",
      " [12/22] [None] attached joint:[None] body:[body_obj_mug_5]\n",
      " [13/22] [None] attached joint:[None] body:[body_obj_mug_5]\n",
      " [14/22] [None] attached joint:[None] body:[body_obj_mug_5]\n",
      " [15/22] [None] attached joint:[None] body:[body_obj_mug_5]\n",
      " [16/22] [None] attached joint:[None] body:[body_obj_plate_11]\n",
      " [17/22] [None] attached joint:[None] body:[body_obj_plate_11]\n",
      " [18/22] [None] attached joint:[None] body:[body_obj_plate_11]\n",
      " [19/22] [None] attached joint:[None] body:[body_obj_plate_11]\n",
      " [20/22] [None] attached joint:[None] body:[body_obj_plate_11]\n",
      " [21/22] [None] attached joint:[None] body:[body_obj_plate_11]\n",
      "\n",
      "Free joint information. n_free_joint:[2]\n",
      " [0/2] [None] body_name_attached:[body_obj_mug_5]\n",
      " [1/2] [None] body_name_attached:[body_obj_plate_11]\n",
      "\n",
      "Revolute joint information. n_rev_joint:[10]\n",
      " [0/10] [joint1] range:[-6.283]~[6.283]\n",
      " [1/10] [joint2] range:[-6.283]~[6.283]\n",
      " [2/10] [joint3] range:[-6.283]~[6.283]\n",
      " [3/10] [joint4] range:[-6.283]~[6.283]\n",
      " [4/10] [joint5] range:[-6.283]~[6.283]\n",
      " [5/10] [joint6] range:[-6.283]~[6.283]\n",
      " [6/10] [rh_r1] range:[0.000]~[1.100]\n",
      " [7/10] [rh_r2] range:[0.000]~[1.000]\n",
      " [8/10] [rh_l1] range:[0.000]~[1.100]\n",
      " [9/10] [rh_l2] range:[0.000]~[1.000]\n",
      "\n",
      "Prismatic joint information. n_pri_joint:[0]\n",
      "\n",
      "Control information. n_ctrl:[10]\n",
      " [0/10] [actuator_joint1] range:[-6.283]~[6.283] gear:[1.00] type:[JOINT]\n",
      " [1/10] [actuator_joint2] range:[-6.283]~[6.283] gear:[1.00] type:[JOINT]\n",
      " [2/10] [actuator_joint3] range:[-6.283]~[6.283] gear:[1.00] type:[JOINT]\n",
      " [3/10] [actuator_joint4] range:[-6.283]~[6.283] gear:[1.00] type:[JOINT]\n",
      " [4/10] [actuator_joint5] range:[-6.283]~[6.283] gear:[1.00] type:[JOINT]\n",
      " [5/10] [actuator_joint6] range:[-6.283]~[6.283] gear:[1.00] type:[JOINT]\n",
      " [6/10] [actuator_rh_r1] range:[0.000]~[1.100] gear:[1.00] type:[JOINT]\n",
      " [7/10] [actuator_rh_r2] range:[0.000]~[1.000] gear:[1.00] type:[JOINT]\n",
      " [8/10] [actuator_rh_l1] range:[0.000]~[1.100] gear:[1.00] type:[JOINT]\n",
      " [9/10] [actuator_rh_l2] range:[0.000]~[1.000] gear:[1.00] type:[JOINT]\n",
      "\n",
      "Camera information. n_cam:[4]\n",
      " [0/4] [agentview] fov:[60.0]\n",
      " [1/4] [topview] fov:[90.0]\n",
      " [2/4] [sideview] fov:[90.0]\n",
      " [3/4] [egocentric] fov:[90.0]\n",
      "\n",
      "n_sensor:[0]\n",
      "sensor_names:[]\n",
      "n_site:[6]\n",
      "site_names:['bottom_site_mug_5', 'top_site_mug_5', 'horizontal_radius_site_mug_5', 'bottom_site_plate_11', 'top_site_plate_11', 'horizontal_radius_site_plate_11']\n",
      "-----------------------------------------------------------------------------\n",
      "env:[Tabletop] reset\n",
      "env:[Tabletop] reset\n",
      "env:[Tabletop] initalize viewer\n",
      "DONE INITIALIZATION\n"
     ]
    }
   ],
   "source": [
    "# 1. 导入MuJoCo环境模块\n",
    "from mujoco_env.y_env import SimpleEnv  # 导入简化的MuJoCo环境接口\n",
    "\n",
    "# 2. 指定场景XML文件路径\n",
    "xml_path = './asset/example_scene_y.xml'  # 场景描述文件，定义了机器人、物体和环境的物理属性\n",
    "\n",
    "# 3. 创建抓取放置环境实例\n",
    "# action_type='joint_angle'表示控制方式为关节角度控制（而非末端执行器位置控制）\n",
    "PnPEnv = SimpleEnv(xml_path, action_type='joint_angle')  # 创建Pick and Place环境\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roll-Out Your Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE INITIALIZATION\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 52\u001b[0m\n\u001b[1;32m     43\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation.state\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([state])\u001b[38;5;241m.\u001b[39mto(device),  \u001b[38;5;66;03m# 机器人状态\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation.image\u001b[39m\u001b[38;5;124m'\u001b[39m: image\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device),  \u001b[38;5;66;03m# 第三人称视角图像\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([step\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m20\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# 时间戳（秒）\u001b[39;00m\n\u001b[1;32m     49\u001b[0m }\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# 3.7 使用策略选择动作\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 根据当前观察选择动作\u001b[39;00m\n\u001b[1;32m     53\u001b[0m action \u001b[38;5;241m=\u001b[39m action[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# 将动作张量转换为NumPy数组\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# 3.8 在环境中执行动作\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/lerobot-mujoco/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/lerobot-mujoco/lib/python3.10/site-packages/lerobot/common/policies/act/modeling_act.py:119\u001b[0m, in \u001b[0;36mACTPolicy.select_action\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Select a single action given environment observations.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03mThis method wraps `select_actions` in order to return one action at a time for execution in the\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03menvironment. It works by managing the actions in a queue and only calling `select_actions` when the\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03mqueue is empty.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 119\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mimage_features:\n\u001b[1;32m    121\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(batch)  \u001b[38;5;66;03m# shallow copy so that adding a key doesn't modify the original\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/lerobot-mujoco/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/lerobot-mujoco/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/envs/lerobot-mujoco/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/lerobot-mujoco/lib/python3.10/site-packages/lerobot/common/policies/normalize.py:171\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(mean)\u001b[38;5;241m.\u001b[39many(), _no_stats_error_str(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(std)\u001b[38;5;241m.\u001b[39many(), _no_stats_error_str(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m     batch[key] \u001b[38;5;241m=\u001b[39m (batch[key] \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m (std \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m norm_mode \u001b[38;5;129;01mis\u001b[39;00m NormalizationMode\u001b[38;5;241m.\u001b[39mMIN_MAX:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mmin\u001b[39m \u001b[38;5;241m=\u001b[39m buffer[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. 初始化环境和策略\n",
    "step = 0  # 初始化步数计数器\n",
    "PnPEnv.reset(seed=0)  # 重置环境，设置随机种子为0以确保可重复性\n",
    "policy.reset()  # 重置策略内部状态\n",
    "policy.eval()  # 将策略设置为评估模式（不进行梯度计算）\n",
    "\n",
    "# 2. 设置图像处理参数\n",
    "save_image = True  # 是否保存图像的标志\n",
    "img_transform = torchvision.transforms.ToTensor()  # 图像转换器，将PIL图像转换为PyTorch张量\n",
    "\n",
    "# 3. 主循环：只要MuJoCo查看器窗口保持打开状态就持续运行\n",
    "while PnPEnv.env.is_viewer_alive():\n",
    "    # 3.1 推进物理仿真一步\n",
    "    PnPEnv.step_env()\n",
    "    \n",
    "    # 3.2 以20Hz的频率执行控制逻辑（每秒20次）\n",
    "    if PnPEnv.env.loop_every(HZ=20):\n",
    "        # 3.3 检查任务是否完成\n",
    "        success = PnPEnv.check_success()\n",
    "        if success:\n",
    "            print('Success')  # 打印成功信息\n",
    "            # 重置环境和策略，准备下一轮任务\n",
    "            policy.reset()\n",
    "            PnPEnv.reset(seed=0)\n",
    "            step = 0\n",
    "            save_image = False\n",
    "            \n",
    "        # 3.4 获取当前环境状态\n",
    "        state = PnPEnv.get_ee_pose()  # 获取机器人末端执行器的位姿\n",
    "        \n",
    "        # 3.5 获取当前环境图像\n",
    "        image, wirst_image = PnPEnv.grab_image()  # 获取第三人称视角和手腕相机图像\n",
    "        # 处理第三人称视角图像\n",
    "        image = Image.fromarray(image)  # 将NumPy数组转换为PIL图像\n",
    "        image = image.resize((256, 256))  # 调整图像大小为256x256\n",
    "        image = img_transform(image)  # 转换为PyTorch张量\n",
    "        # 处理手腕相机图像\n",
    "        wrist_image = Image.fromarray(wirst_image)\n",
    "        wrist_image = wrist_image.resize((256, 256))\n",
    "        wrist_image = img_transform(wrist_image)\n",
    "        \n",
    "        # 3.6 构建输入数据字典\n",
    "        data = {\n",
    "            'observation.state': torch.tensor([state]).to(device),  # 机器人状态\n",
    "            'observation.image': image.unsqueeze(0).to(device),  # 第三人称视角图像\n",
    "            'observation.wrist_image': wrist_image.unsqueeze(0).to(device),  # 手腕相机图像\n",
    "            'task': ['Put mug cup on the plate'],  # 任务描述\n",
    "            'timestamp': torch.tensor([step/20]).to(device)  # 时间戳（秒）\n",
    "        }\n",
    "        \n",
    "        # 3.7 使用策略选择动作\n",
    "        action = policy.select_action(data)  # 根据当前观察选择动作\n",
    "        action = action[0].cpu().detach().numpy()  # 将动作张量转换为NumPy数组\n",
    "        \n",
    "        # 3.8 在环境中执行动作\n",
    "        _ = PnPEnv.step(action)  # 执行选定的动作\n",
    "        PnPEnv.render()  # 渲染环境\n",
    "        \n",
    "        # 3.9 更新步数计数器并检查任务完成情况\n",
    "        step += 1\n",
    "        success = PnPEnv.check_success()\n",
    "        if success:\n",
    "            print('Success')  # 打印成功信息\n",
    "            break  # 任务完成，退出循环\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PnPEnv.env.close_viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lerobot-mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
