#!/bin/bash
# ===================================================
# Optimized Training Script with Tokenizer Fix
# ä¼˜åŒ–çš„è®­ç»ƒè„šæœ¬ï¼ˆä¿®å¤tokenizeré—®é¢˜ï¼‰
# ===================================================

echo "=================================================="
echo "ğŸš€ Starting Optimized Training | å¯åŠ¨ä¼˜åŒ–è®­ç»ƒ"
echo "=================================================="

# 1. Fix tokenizer parallelism warning
# ä¿®å¤tokenizerå¹¶è¡ŒåŒ–è­¦å‘Š
export TOKENIZERS_PARALLELISM=false
echo "âœ“ TOKENIZERS_PARALLELISM=false"

# 2. Set optimal environment variables
# è®¾ç½®æœ€ä¼˜ç¯å¢ƒå˜é‡
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=4
echo "âœ“ Thread optimization applied | çº¿ç¨‹ä¼˜åŒ–å·²åº”ç”¨"

# 3. Clear Python cache (optional but recommended)
# æ¸…ç†Pythonç¼“å­˜ï¼ˆå¯é€‰ä½†æ¨èï¼‰
find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
echo "âœ“ Cache cleared | ç¼“å­˜å·²æ¸…ç†"

# 4. Resume training from checkpoint
# ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
echo ""
echo "Starting training... | å¼€å§‹è®­ç»ƒ..."
echo ""

python train_model.py --config_path smolvla_omy.yaml

echo ""
echo "=================================================="
echo "âœ… Training Complete | è®­ç»ƒå®Œæˆ"
echo "=================================================="
